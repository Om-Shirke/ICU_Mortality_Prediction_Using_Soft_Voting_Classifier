{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b1ef88",
   "metadata": {},
   "source": [
    "### Notebook Overview\n",
    "This notebook contains the complete comparision the with state -of-art models. \n",
    "<br>This comparision of model performance is also used to analyse the effect of scaling and balancing. Standard Scaling is applied and class balancing using **ADASYN** is applied here.\n",
    "<br>*Multiple scaling techniques were  analysed to check which best improved the state-of-art models' performances. Multiple class balancing techniques were also implemented analyse the state-of-art models' performances, and then ADASYN balancing technique was chosen.*\n",
    "<br>*However, these experiments with different scaling techniques and balancing techniques implementations are not included here due to compuatational limitations and need for a comprehensive coding notebook.* \n",
    "<br>For the remaining steps in this study's implementation, such as scaling and class balancing, please refer to the other notebooks included in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d9c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tpot import TPOTClassifier\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c58fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"prepared_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a3b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0177c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fceeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e4464",
   "metadata": {},
   "outputs": [],
   "source": [
    " df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d65162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7a1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04839a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spliting of the dataset\n",
    "X=df.drop(\"outcome\",axis=1)\n",
    "y=df[\"outcome\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets (70-30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "\n",
    "\n",
    "# Initializing models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"SVM\": SVC(probability=True, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(random_state=42, verbose=0),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42, verbose=-1),  # Suppress LightGBM outputs\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42, algorithm=\"SAMME\"),  # Set algorithm to SAMME\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"k-NN\": KNeighborsClassifier(),\n",
    "    \"LDA\": LinearDiscriminantAnalysis()\n",
    "}\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions on testing data\n",
    "    test_preds = model.predict(X_test)\n",
    "    test_probs = model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(y_test, test_preds)\n",
    "    \n",
    "    \n",
    "    class_report_str = classification_report(y_test, test_preds, target_names=['Class 0', 'Class 1'])\n",
    "    class_report = classification_report(y_test, test_preds, target_names=['Class 0', 'Class 1'], output_dict=True)\n",
    "    \n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, test_preds)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    \n",
    "    \n",
    "    sensitivity_1 = tp / (tp + fn)\n",
    "    specificity_1 = tn / (tn + fp)\n",
    "    \n",
    "    \n",
    "    auc_roc = roc_auc_score(y_test, test_probs)\n",
    "    \n",
    "   \n",
    "    metrics = {\n",
    "        \"Class 1\": {\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Sensitivity\": sensitivity_1,\n",
    "            \"Specificity\": specificity_1,\n",
    "            \"Precision\": class_report['Class 1']['precision'],\n",
    "            \"Recall\": class_report['Class 1']['recall'],\n",
    "            \"F1-Score\": class_report['Class 1']['f1-score'],\n",
    "            \"AUC ROC Score\": auc_roc\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    results[name] = (metrics, class_report_str)\n",
    "\n",
    "# Printing the results\n",
    "for model_name, (metrics, class_report_str) in results.items():\n",
    "    print(f\"**For {model_name}:**\")\n",
    "    for cls, cls_metrics in metrics.items():\n",
    "        print(f\"Metrics for {cls}:\")\n",
    "        for metric, value in cls_metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    print(f\"Classification Report - Test Data for {model_name}:\")\n",
    "    print(class_report_str)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49392145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515fc02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc34ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = ['Model', 'Accuracy', 'Recall', 'Precision', 'Sensitivity', 'Specificity', 'F1-Score', 'AUC ROC Score']\n",
    "df_results = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Filling the list with the results\n",
    "for model_name, (metrics, _) in results.items():\n",
    "    row = [\n",
    "        model_name,\n",
    "        metrics['Class 1']['Accuracy'],\n",
    "        metrics['Class 1']['Recall'],\n",
    "        metrics['Class 1']['Precision'],\n",
    "        metrics['Class 1']['Sensitivity'],\n",
    "        metrics['Class 1']['Specificity'],\n",
    "        metrics['Class 1']['F1-Score'],\n",
    "        metrics['Class 1']['AUC ROC Score']\n",
    "    ]\n",
    "    rows.append(pd.Series(row, index=columns))\n",
    "\n",
    "\n",
    "df_results = pd.concat(rows, axis=1).T  # Transpose since Series are concatenated along the columns\n",
    "\n",
    "\n",
    "df_results.set_index('Model', inplace=True)\n",
    "\n",
    "# Styling the DataFrame\n",
    "styled_table = (\n",
    "    df_results.style\n",
    "    .set_table_styles([\n",
    "        {'selector': 'th', 'props': [('border', '2px solid black')]},  # Bold border for headers\n",
    "        {'selector': 'td', 'props': [('border', '1px solid black')]},  # Ordinary border for other cells\n",
    "        {'selector': 'th.col_heading.level0', 'props': [('border', '2px solid black')]},  # Bold border for metric names\n",
    "    ])\n",
    "    .set_properties(**{'border': '1px solid black'})  # Ordinary border for all cells\n",
    "    .set_table_attributes('style=\"border-collapse:collapse\"')\n",
    ")\n",
    "\n",
    "\n",
    "styled_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595641bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = ['Model', 'Accuracy', 'Recall', 'Precision', 'Sensitivity', 'Specificity', 'F1-Score', 'AUC ROC Score']\n",
    "df_results = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Adding the list with the results\n",
    "for model_name, (metrics, _) in results.items():\n",
    "    row = [\n",
    "        model_name,\n",
    "        metrics['Class 1']['Accuracy'],\n",
    "        metrics['Class 1']['Recall'],\n",
    "        metrics['Class 1']['Precision'],\n",
    "        metrics['Class 1']['Sensitivity'],\n",
    "        metrics['Class 1']['Specificity'],\n",
    "        metrics['Class 1']['F1-Score'],\n",
    "        metrics['Class 1']['AUC ROC Score']\n",
    "    ]\n",
    "    rows.append(pd.Series(row, index=columns))\n",
    "\n",
    "\n",
    "df_results = pd.concat(rows, axis=1).T  # Transpose since Series are concatenated along the columns\n",
    "\n",
    "\n",
    "df_results.set_index('Model', inplace=True)\n",
    "\n",
    "# Styling the DataFrame\n",
    "styled_table = (\n",
    "    df_results.style\n",
    "    .format('{:.4f}')  # Format all cells to 4 decimal places\n",
    "    .set_table_styles([\n",
    "        {'selector': 'th', 'props': [('border', '2px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt'), ('font-weight', 'bold')]},  # Bold border and styling for headers\n",
    "        {'selector': 'td', 'props': [('border', '1px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt')]},  # Ordinary border and styling for cells\n",
    "        {'selector': 'th.col_heading.level0', 'props': [('border', '2px solid black')]},  # Bold border for metric names\n",
    "    ])\n",
    "    .set_properties(**{'border': '1px solid black'})  # Ordinary border for all cells\n",
    "    .set_table_attributes('style=\"border-collapse:collapse\"')\n",
    ")\n",
    "\n",
    "styled_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47534613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed65b36b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "407f47a7",
   "metadata": {},
   "source": [
    "### Post Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0342a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e7656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d09f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c345a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Categorical=X[['hypertensive','atrialfibrillation', 'diabetes', 'deficiencyanemias','depression', \n",
    "                 'Hyperlipemia', 'Renal_failure', 'COPD','gendera']]\n",
    "X_Categorical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796706be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Numerical= X.drop(columns=X_Categorical.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3932b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Numerical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e7f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fitting and transform the numerical features\n",
    "X_Numerical_scaled = scaler.fit_transform(X_Numerical)\n",
    "\n",
    "\n",
    "X_Numerical_scaled_df = pd.DataFrame(X_Numerical_scaled, columns=X_Numerical.columns, index=X_Numerical.index)\n",
    "\n",
    "\n",
    "X_scaled = pd.concat([X_Categorical, X_Numerical_scaled_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46852762",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c98c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c17939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets (70-30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5a53fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "\n",
    "\n",
    "# Initializing models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"SVM\": SVC(probability=True, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(random_state=42, verbose=0),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42, verbose=-1),  # Suppress LightGBM outputs\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42, algorithm=\"SAMME\"),  # Set algorithm to SAMME\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"k-NN\": KNeighborsClassifier(),\n",
    "    \"LDA\": LinearDiscriminantAnalysis()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    test_preds = model.predict(X_test)\n",
    "    test_probs = model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "    \n",
    " \n",
    "    accuracy = accuracy_score(y_test, test_preds)\n",
    "    \n",
    "    \n",
    "    class_report_str = classification_report(y_test, test_preds, target_names=['Class 0', 'Class 1'])\n",
    "    class_report = classification_report(y_test, test_preds, target_names=['Class 0', 'Class 1'], output_dict=True)\n",
    "    \n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, test_preds)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    \n",
    "    \n",
    "    sensitivity_1 = tp / (tp + fn)\n",
    "    specificity_1 = tn / (tn + fp)\n",
    "    \n",
    "    \n",
    "    auc_roc = roc_auc_score(y_test, test_probs)\n",
    "    \n",
    "    # Arranging output metrics in a systematic form for Class 1\n",
    "    metrics = {\n",
    "        \"Class 1\": {\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Sensitivity\": sensitivity_1,\n",
    "            \"Specificity\": specificity_1,\n",
    "            \"Precision\": class_report['Class 1']['precision'],\n",
    "            \"Recall\": class_report['Class 1']['recall'],\n",
    "            \"F1-Score\": class_report['Class 1']['f1-score'],\n",
    "            \"AUC ROC Score\": auc_roc\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    results[name] = (metrics, class_report_str)\n",
    "\n",
    "# Printing the results\n",
    "for model_name, (metrics, class_report_str) in results.items():\n",
    "    print(f\"**For {model_name}:**\")\n",
    "    for cls, cls_metrics in metrics.items():\n",
    "        print(f\"Metrics for {cls}:\")\n",
    "        for metric, value in cls_metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "   \n",
    "    print(f\"Classification Report - Test Data for {model_name}:\")\n",
    "    print(class_report_str)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2aa462",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = ['Model', 'Accuracy', 'Recall', 'Precision', 'Sensitivity', 'Specificity', 'F1-Score', 'AUC ROC Score']\n",
    "df_results = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Filling the list with the results\n",
    "for model_name, (metrics, _) in results.items():\n",
    "    row = [\n",
    "        model_name,\n",
    "        metrics['Class 1']['Accuracy'],\n",
    "        metrics['Class 1']['Recall'],\n",
    "        metrics['Class 1']['Precision'],\n",
    "        metrics['Class 1']['Sensitivity'],\n",
    "        metrics['Class 1']['Specificity'],\n",
    "        metrics['Class 1']['F1-Score'],\n",
    "        metrics['Class 1']['AUC ROC Score']\n",
    "    ]\n",
    "    rows.append(pd.Series(row, index=columns))\n",
    "\n",
    "\n",
    "df_results = pd.concat(rows, axis=1).T  # Transpose since Series are concatenated along the columns\n",
    "\n",
    "\n",
    "df_results.set_index('Model', inplace=True)\n",
    "\n",
    "# Styling the DataFrame\n",
    "styled_table = (\n",
    "    df_results.style\n",
    "    .format('{:.4f}')  # Format all cells to 4 decimal places\n",
    "    .set_table_styles([\n",
    "        {'selector': 'th', 'props': [('border', '2px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt'), ('font-weight', 'bold')]},  # Bold border and styling for headers\n",
    "        {'selector': 'td', 'props': [('border', '1px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt')]},  # Ordinary border and styling for cells\n",
    "        {'selector': 'th.col_heading.level0', 'props': [('border', '2px solid black')]},  # Bold border for metric names\n",
    "    ])\n",
    "    .set_properties(**{'border': '1px solid black'})  # Ordinary border for all cells\n",
    "    .set_table_attributes('style=\"border-collapse:collapse\"')\n",
    ")\n",
    "\n",
    "\n",
    "styled_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6394ba12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "467364a2",
   "metadata": {},
   "source": [
    "##  Post Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc4bf9f",
   "metadata": {},
   "source": [
    "* *Multiple Balancing Techniques were implemented to check which optimised the performance of the benchmark models the best.*\n",
    "* *These experiments with different scaling balancing techniques implementationsand analysis are not included here due to compuatational limitations and need for a comprehensive coding notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a536db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "df_y = pd.DataFrame({'target': y})\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "# Creating the plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='target', data=df_y, palette='pastel')\n",
    "\n",
    "\n",
    "plt.title('Class Imbalance of the Target Variable', fontsize=16)\n",
    "plt.xlabel('Class', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d318da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets (70-30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c20e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "\n",
    "# Initialize ADASYN\n",
    "adasyn = ADASYN(random_state=42)\n",
    "\n",
    "# Resample the dataset\n",
    "X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Check the class distribution after resampling\n",
    "print(\"Class distribution after ADASYN:\")\n",
    "print(y_train_resampled.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d125cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train_resampled\n",
    "y_train=y_train_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981d62d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
    "\n",
    "# Initializing models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"SVM\": SVC(probability=True, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(random_state=42, verbose=0),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42, verbose=-1),  # Suppress LightGBM outputs\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42, algorithm=\"SAMME\"),  # Set algorithm to SAMME\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"k-NN\": KNeighborsClassifier(),\n",
    "    \"LDA\": LinearDiscriminantAnalysis()\n",
    "}\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Iterating over models\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions on testing data\n",
    "    test_preds = model.predict(X_test)\n",
    "    test_probs = model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "    \n",
    "    # Calculating accuracy\n",
    "    accuracy = accuracy_score(y_test, test_preds)\n",
    "    \n",
    "    # Generating classification report for test data\n",
    "    class_report_str = classification_report(y_test, test_preds, target_names=['Class 0', 'Class 1'])\n",
    "    class_report = classification_report(y_test, test_preds, target_names=['Class 0', 'Class 1'], output_dict=True)\n",
    "    \n",
    "    # Confusion matrix to calculate sensitivity and specificity\n",
    "    conf_matrix = confusion_matrix(y_test, test_preds)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    \n",
    "    # Calculating Sensitivity and Specificity for Class 1\n",
    "    sensitivity_1 = tp / (tp + fn)\n",
    "    specificity_1 = tn / (tn + fp)\n",
    "    \n",
    "    # Calculatong AUC ROC Score\n",
    "    auc_roc = roc_auc_score(y_test, test_probs)\n",
    "    \n",
    "    # Arranging output metrics in a systematic form for Class 1\n",
    "    metrics = {\n",
    "        \"Class 1\": {\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Sensitivity\": sensitivity_1,\n",
    "            \"Specificity\": specificity_1,\n",
    "            \"Precision\": class_report['Class 1']['precision'],\n",
    "            \"Recall\": class_report['Class 1']['recall'],\n",
    "            \"F1-Score\": class_report['Class 1']['f1-score'],\n",
    "            \"AUC ROC Score\": auc_roc\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    results[name] = (metrics, class_report_str)\n",
    "\n",
    "# Printing the results\n",
    "for model_name, (metrics, class_report_str) in results.items():\n",
    "    print(f\"**For {model_name}:**\")\n",
    "    for cls, cls_metrics in metrics.items():\n",
    "        print(f\"Metrics for {cls}:\")\n",
    "        for metric, value in cls_metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    print(f\"Classification Report - Test Data for {model_name}:\")\n",
    "    print(class_report_str)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca806781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an empty DataFrame with the required column names\n",
    "columns = ['Model', 'Accuracy', 'Recall', 'Precision', 'Sensitivity', 'Specificity', 'F1-Score', 'AUC ROC Score']\n",
    "df_results = pd.DataFrame(columns=columns)\n",
    "\n",
    "# List to collect rows for concatenation\n",
    "rows = []\n",
    "\n",
    "# Populate the list with the results\n",
    "for model_name, (metrics, _) in results.items():\n",
    "    row = [\n",
    "        model_name,\n",
    "        metrics['Class 1']['Accuracy'],\n",
    "        metrics['Class 1']['Recall'],\n",
    "        metrics['Class 1']['Precision'],\n",
    "        metrics['Class 1']['Sensitivity'],\n",
    "        metrics['Class 1']['Specificity'],\n",
    "        metrics['Class 1']['F1-Score'],\n",
    "        metrics['Class 1']['AUC ROC Score']\n",
    "    ]\n",
    "    rows.append(pd.Series(row, index=columns))\n",
    "\n",
    "# Concatenating all rows into the DataFrame\n",
    "df_results = pd.concat(rows, axis=1).T  # Transpose since Series are concatenated along the columns\n",
    "\n",
    "\n",
    "df_results.set_index('Model', inplace=True)\n",
    "\n",
    "# Styling the DataFrame\n",
    "styled_table = (\n",
    "    df_results.style\n",
    "    .format('{:.4f}')  # Format all cells to 4 decimal places\n",
    "    .set_table_styles([\n",
    "        {'selector': 'th', 'props': [('border', '2px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt'), ('font-weight', 'bold')]},  # Bold border and styling for headers\n",
    "        {'selector': 'td', 'props': [('border', '1px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt')]},  # Ordinary border and styling for cells\n",
    "        {'selector': 'th.col_heading.level0', 'props': [('border', '2px solid black')]},  # Bold border for metric names\n",
    "    ])\n",
    "    .set_properties(**{'border': '1px solid black'})  # Ordinary border for all cells\n",
    "    .set_table_attributes('style=\"border-collapse:collapse\"')\n",
    ")\n",
    "\n",
    "\n",
    "styled_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eed098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab07fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
