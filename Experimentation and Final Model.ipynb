{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c7e839",
   "metadata": {},
   "source": [
    "This notebook consists of various experiments performed during experimentation stage of the voting classifier. \n",
    "<br>**Note:** *Close to 50-60 experiments were  conducted to get the final classifier model*. \n",
    "<br>They are not plausible to add in a single notebook, that would required multiple notebooks, that don't particulary convey anything.\n",
    "<br>Since, they are computationally exhautive to run, *only the most significant experiments*, \"which are also mentioned in the dissertation file's analysis*, are included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d9c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tpot import TPOTClassifier\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c58fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"prepared_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a3b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0177c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fceeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05dd915",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spliting of the dataset\n",
    "X=df.drop(\"outcome\",axis=1)\n",
    "y=df[\"outcome\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73e3a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets (70-30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f47a7",
   "metadata": {},
   "source": [
    "### Post Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0342a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e7656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d09f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c345a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Categorical=X[['hypertensive','atrialfibrillation', 'diabetes', 'deficiencyanemias','depression', \n",
    "                 'Hyperlipemia', 'Renal_failure', 'COPD','gendera']]\n",
    "X_Categorical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796706be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Numerical= X.drop(columns=X_Categorical.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3932b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Numerical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e7f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the numerical features\n",
    "X_Numerical_scaled = scaler.fit_transform(X_Numerical)\n",
    "\n",
    "\n",
    "X_Numerical_scaled_df = pd.DataFrame(X_Numerical_scaled, columns=X_Numerical.columns, index=X_Numerical.index)\n",
    "\n",
    "\n",
    "X_scaled = pd.concat([X_Categorical, X_Numerical_scaled_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46852762",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c98c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c17939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets (70-30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6394ba12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "467364a2",
   "metadata": {},
   "source": [
    "##  Post Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a536db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "df_y = pd.DataFrame({'target': y})\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "# Creating the plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='target', data=df_y, palette='pastel')\n",
    "\n",
    "\n",
    "plt.title('Class Imbalance of the Target Variable', fontsize=16)\n",
    "plt.xlabel('Class', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d318da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets (70-30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c20e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Assuming X_scaled is your scaled feature matrix and y is your target variable\n",
    "\n",
    "# Initializing ADASYN\n",
    "adasyn = ADASYN(random_state=42)\n",
    "\n",
    "# Resample=ing the dataset\n",
    "X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Class distribution after ADASYN:\")\n",
    "print(y_train_resampled.value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d125cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train_resampled\n",
    "y_train=y_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d0e3c2",
   "metadata": {},
   "source": [
    "## Experimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c994be13",
   "metadata": {},
   "source": [
    "* *Close to 50-60 experiments were  conducted to get the final classifier model*. \n",
    "* Since, they are computationally exhautive to run, *only the most significant experiments*, \"which are also mentioned in the dissertation file's analysis*, are included here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcecaed",
   "metadata": {},
   "source": [
    "**Note:** The in-depth analysis included in the dissertation with comprehensively exhibited output are done manually using MS Excel. <br>Only basic tables are mentioned in these notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11756aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Define the parameter grid for each model\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'scale_pos_weight': [1, 2, 5]\n",
    "}\n",
    "\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [5, 10, 15],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "nb_param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "}\n",
    "\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "# Define the parameter grid for LDA\n",
    "lda_param_grid = {\n",
    "    'solver': ['svd', 'lsqr', 'eigen'],\n",
    "    'shrinkage': [None, 'auto']  # Only applicable for 'lsqr' and 'eigen' solvers\n",
    "}\n",
    "\n",
    "# Initialize all models with default parameters\n",
    "svm_model = SVC(probability=True, random_state=42)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "knn_model = KNeighborsClassifier()\n",
    "nb_model = GaussianNB()\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "lda_model = LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f37e7",
   "metadata": {},
   "source": [
    "### 2 Base learners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22a2f92",
   "metadata": {},
   "source": [
    "**$$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7eb57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Suppress XGBoost warnings\n",
    "logging.getLogger('xgboost').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# Grid search for KNN\n",
    "knn_grid = GridSearchCV(knn_model, param_grid=knn_param_grid, scoring='recall', cv=3)\n",
    "knn_grid.fit(X_train, y_train)\n",
    "knn_model_best = knn_grid.best_estimator_\n",
    "\n",
    "# Grid search for LDA\n",
    "lda_grid = GridSearchCV(lda_model, param_grid=lda_param_grid, scoring='recall', cv=3)\n",
    "lda_grid.fit(X_train, y_train)\n",
    "lda_model_best = lda_grid.best_estimator_\n",
    "\n",
    "# Defining the Voting Classifier with the selected base learners\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('knn', knn_model_best),\n",
    "        ('lda', lda_model_best)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    weights=[1, 1]\n",
    ")\n",
    "\n",
    "# Training the Voting Classifier on the full training data\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting probabilities and adjust the threshold for the test data\n",
    "voting_probs = voting_clf.predict_proba(X_test)[:, 1]\n",
    "threshold = 0.4\n",
    "voting_preds = (voting_probs > threshold).astype(int)\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_test, voting_preds)\n",
    "recall = recall_score(y_test, voting_preds)\n",
    "precision = precision_score(y_test, voting_preds)\n",
    "f1 = f1_score(y_test, voting_preds)\n",
    "auc_roc = roc_auc_score(y_test, voting_probs)\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, voting_preds)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"AUC ROC Score: {auc_roc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981864ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Metrics obtained from the executed code\n",
    "metrics = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Sensitivity': sensitivity,\n",
    "    'Specificity': specificity,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'AUC ROC Score': auc_roc\n",
    "}\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame(list(metrics.items()), columns=['Metric', 'Value'])\n",
    "\n",
    "# Styling the DataFrame\n",
    "styled_table = (\n",
    "    df_metrics.style\n",
    "    .set_table_styles([\n",
    "        {'selector': 'th', 'props': [('border', '2px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt'), ('font-weight', 'bold')]},  # Bold border and styling for headers\n",
    "        {'selector': 'td', 'props': [('border', '1px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt')]},  # Ordinary border and styling for cells\n",
    "        {'selector': 'caption', 'props': [('caption-side', 'top'), ('font-size', '14pt'), ('font-weight', 'bold'), ('color', 'black'), ('font-family', 'Times New Roman')]},  # Style for caption\n",
    "    ])\n",
    "    .set_properties(**{'border': '1px solid black'})  # Border for all cells\n",
    "    .set_caption(\"2 Base Learners: KNN and LDA\")  # Add a caption to the table\n",
    ")\n",
    "\n",
    "\n",
    "styled_table = styled_table.hide(axis=\"index\")\n",
    "\n",
    "\n",
    "styled_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd14b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55265f6b",
   "metadata": {},
   "source": [
    "**$$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d816e27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Suppress XGBoost warnings\n",
    "logging.getLogger('xgboost').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# Grid search for SVM\n",
    "svm_grid = GridSearchCV(svm_model, param_grid=svm_param_grid, scoring='recall', cv=3)\n",
    "svm_grid.fit(X_train, y_train)\n",
    "svm_model_best = svm_grid.best_estimator_\n",
    "\n",
    "# Grid search for RandomForest\n",
    "rf_grid = GridSearchCV(rf_model, param_grid=rf_param_grid, scoring='recall', cv=3)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "rf_model_best = rf_grid.best_estimator_\n",
    "\n",
    "# Defining the Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('svm', svm_model_best),\n",
    "        ('rf', rf_model_best)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    weights=[1, 1]\n",
    ")\n",
    "\n",
    "# Train the Voting Classifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities and adjust the threshold\n",
    "voting_probs = voting_clf.predict_proba(X_test)[:, 1]\n",
    "threshold = 0.4\n",
    "voting_preds = (voting_probs > threshold).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, voting_preds)\n",
    "recall = recall_score(y_test, voting_preds)\n",
    "precision = precision_score(y_test, voting_preds)\n",
    "f1 = f1_score(y_test, voting_preds)\n",
    "auc_roc = roc_auc_score(y_test, voting_probs)\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, voting_preds)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"AUC ROC Score: {auc_roc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d9bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c978b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    \"Metric\": [\"Accuracy\", \"Sensitivity\", \"Specificity\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC ROC Score\"],\n",
    "    \"Value\": [accuracy, sensitivity, specificity, precision, recall, f1, auc_roc]\n",
    "}\n",
    "\n",
    "# Creating a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "\n",
    "print(metrics_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Metrics obtained from the above executed code\n",
    "metrics = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Sensitivity': sensitivity,\n",
    "    'Specificity': specificity,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'AUC ROC Score': auc_roc\n",
    "}\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame(list(metrics.items()), columns=['Metric', 'Value'])\n",
    "\n",
    "# Styling the DataFrame\n",
    "styled_table = (\n",
    "    df_metrics.style\n",
    "    .set_table_styles([\n",
    "        {'selector': 'th', 'props': [('border', '2px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt'), ('font-weight', 'bold')]},  # Bold border and styling for headers\n",
    "        {'selector': 'td', 'props': [('border', '1px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt')]},  # Ordinary border and styling for cells\n",
    "    ])\n",
    "    .set_properties(**{'border': '1px solid black'})  # Border for all cells\n",
    "    .set_caption(\"2 Base Learners: SVM and Random Forest\")  # Add a caption to the table\n",
    ")\n",
    "\n",
    "\n",
    "styled_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f9bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Metrics obtained from the above executed code\n",
    "metrics = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Sensitivity': sensitivity,\n",
    "    'Specificity': specificity,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'AUC ROC Score': auc_roc\n",
    "}\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame(list(metrics.items()), columns=['Metric', 'Value'])\n",
    "\n",
    "# Style the DataFrame\n",
    "styled_table = (\n",
    "    df_metrics.style\n",
    "    .set_table_styles([\n",
    "        {'selector': 'th', 'props': [('border', '2px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt'), ('font-weight', 'bold')]},  # Bold border and styling for headers\n",
    "        {'selector': 'td', 'props': [('border', '1px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt')]},  # Ordinary border and styling for cells\n",
    "        {'selector': 'caption', 'props': [('caption-side', 'top'), ('font-size', '14pt'), ('font-weight', 'bold'), ('color', 'black'), ('font-family', 'Times New Roman')]},  # Style for caption\n",
    "    ])\n",
    "    .set_properties(**{'border': '1px solid black'})  # Border for all cells\n",
    "    .set_caption(\"2 Base Learners: SVM and Random Forest\")  # Add a caption to the table\n",
    ")\n",
    "\n",
    "\n",
    "styled_table = styled_table.hide(axis=\"index\")\n",
    "\n",
    "\n",
    "styled_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef08a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "metric_names = [\"Accuracy\", \"Sensitivity\", \"Specificity\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC ROC Score\"]\n",
    "metric_values = [accuracy, sensitivity, specificity, precision, recall, f1, auc_roc]\n",
    "\n",
    "# Plot the metrics\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.barh(metric_names, metric_values, color='skyblue')\n",
    "plt.xlabel('Metric Value')\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfce09c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182317b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e91a22a0",
   "metadata": {},
   "source": [
    "**$$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6168826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import logging\n",
    "\n",
    "# Suppress XGBoost warnings\n",
    "logging.getLogger('xgboost').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "# Grid search for XGBoost\n",
    "xgb_grid = GridSearchCV(xgb_model, param_grid=xgb_param_grid, scoring='recall', cv=3)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "xgb_model_best = xgb_grid.best_estimator_\n",
    "\n",
    "# Grid search for k-NN\n",
    "knn_grid = GridSearchCV(knn_model, param_grid=knn_param_grid, scoring='recall', cv=3)\n",
    "knn_grid.fit(X_train, y_train)\n",
    "knn_model_best = knn_grid.best_estimator_\n",
    "\n",
    "# Define the Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_model_best),\n",
    "        ('knn', knn_model_best)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    weights=[1, 1]\n",
    ")\n",
    "\n",
    "# Train the Voting Classifier on the entire training set\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities and adjust the threshold on the test set\n",
    "voting_probs = voting_clf.predict_proba(X_test)[:, 1]\n",
    "threshold = 0.4\n",
    "voting_preds = (voting_probs > threshold).astype(int)\n",
    "\n",
    "# Calculate metrics on the test set\n",
    "accuracy = accuracy_score(y_test, voting_preds)\n",
    "recall = recall_score(y_test, voting_preds)\n",
    "precision = precision_score(y_test, voting_preds)\n",
    "f1 = f1_score(y_test, voting_preds)\n",
    "auc_roc = roc_auc_score(y_test, voting_probs)\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, voting_preds)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "\n",
    "print(f\"Test Set Evaluation:\")\n",
    "print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "print(f\"  Sensitivity: {sensitivity:.4f}\")\n",
    "print(f\"  Specificity: {specificity:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall: {recall:.4f}\")\n",
    "print(f\"  F1-Score: {f1:.4f}\")\n",
    "print(f\"  AUC ROC Score: {auc_roc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08bd900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Metrics obtained from above executed code\n",
    "metrics = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Sensitivity': sensitivity,\n",
    "    'Specificity': specificity,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'AUC ROC Score': auc_roc\n",
    "}\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame(list(metrics.items()), columns=['Metric', 'Value'])\n",
    "\n",
    "# Style the DataFrame\n",
    "styled_table = (\n",
    "    df_metrics.style\n",
    "    .set_table_styles([\n",
    "        {'selector': 'th', 'props': [('border', '2px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt'), ('font-weight', 'bold')]},  # Bold border and styling for headers\n",
    "        {'selector': 'td', 'props': [('border', '1px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt')]},  # Ordinary border and styling for cells\n",
    "        {'selector': 'caption', 'props': [('caption-side', 'top'), ('font-size', '14pt'), ('font-weight', 'bold'), ('color', 'black'), ('font-family', 'Times New Roman')]},  # Style for caption\n",
    "    ])\n",
    "    .set_properties(**{'border': '1px solid black'})  # Border for all cells\n",
    "    .set_caption(\"2 Base Learners: XGBooost and KNN\")  # Add a caption to the table\n",
    ")\n",
    "\n",
    "# Remove the index column (row numbers)\n",
    "styled_table = styled_table.hide(axis=\"index\")\n",
    "\n",
    "\n",
    "styled_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03643055",
   "metadata": {},
   "source": [
    "### With 3 Base Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75d7608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import logging\n",
    "\n",
    "# Suppress warnings\n",
    "logging.getLogger('xgboost').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "# Grid search for KNN\n",
    "knn_grid = GridSearchCV(knn_model, param_grid=knn_param_grid, scoring='recall', cv=3)\n",
    "knn_grid.fit(X_train, y_train)\n",
    "knn_model_best = knn_grid.best_estimator_\n",
    "\n",
    "# Grid search for RandomForest\n",
    "rf_grid = GridSearchCV(rf_model, param_grid=rf_param_grid, scoring='recall', cv=3)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "rf_model_best = rf_grid.best_estimator_\n",
    "\n",
    "# Grid search for XGBoost\n",
    "xgb_grid = GridSearchCV(xgb_model, param_grid=xgb_param_grid, scoring='recall', cv=3)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "xgb_model_best = xgb_grid.best_estimator_\n",
    "\n",
    "# Define the Voting Classifier with the selected base learners\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('knn', knn_model_best),\n",
    "        ('rf', rf_model_best),\n",
    "        ('xgb', xgb_model_best)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    weights=[1, 1, 2]\n",
    ")\n",
    "\n",
    "# Train the Voting Classifier on the full training data\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities and adjust the custom threshold for the test data\n",
    "voting_probs = voting_clf.predict_proba(X_test)[:, 1]\n",
    "threshold = 0.4\n",
    "voting_preds = (voting_probs > threshold).astype(int)\n",
    "\n",
    "# Calculate metrics for the test data\n",
    "accuracy = accuracy_score(y_test, voting_preds)\n",
    "recall = recall_score(y_test, voting_preds)\n",
    "precision = precision_score(y_test, voting_preds)\n",
    "f1 = f1_score(y_test, voting_preds)\n",
    "auc_roc = roc_auc_score(y_test, voting_probs)\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, voting_preds)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Output the metrics \n",
    "print(f\"Test Metrics:\")\n",
    "print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "print(f\"  Sensitivity: {sensitivity:.4f}\")\n",
    "print(f\"  Specificity: {specificity:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall: {recall:.4f}\")\n",
    "print(f\"  F1-Score: {f1:.4f}\")\n",
    "print(f\"  AUC ROC Score: {auc_roc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ae34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Metrics obtained from the executed code\n",
    "metrics = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Sensitivity': sensitivity,\n",
    "    'Specificity': specificity,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'AUC ROC Score': auc_roc\n",
    "}\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame(list(metrics.items()), columns=['Metric', 'Value'])\n",
    "\n",
    "# Style the DataFrame\n",
    "styled_table = (\n",
    "    df_metrics.style\n",
    "    .set_table_styles([\n",
    "        {'selector': 'th', 'props': [('border', '2px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt'), ('font-weight', 'bold')]},  # Bold border and styling for headers\n",
    "        {'selector': 'td', 'props': [('border', '1px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt')]},  # Ordinary border and styling for cells\n",
    "        {'selector': 'caption', 'props': [('caption-side', 'top'), ('font-size', '14pt'), ('font-weight', 'bold'), ('color', 'black'), ('font-family', 'Times New Roman')]},  # Style for caption\n",
    "    ])\n",
    "    .set_properties(**{'border': '1px solid black'})  # Border for all cells\n",
    "    .set_caption(\"3 Base Learners: RF, XGBooost and KNN\")  # Add a caption to the table\n",
    ")\n",
    "\n",
    "# Remove the index column (row numbers)\n",
    "styled_table = styled_table.hide(axis=\"index\")\n",
    "\n",
    "\n",
    "styled_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import logging\n",
    "\n",
    "# Suppress warnings\n",
    "logging.getLogger('xgboost').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# Grid search for SVM\n",
    "svm_grid = GridSearchCV(svm_model, param_grid=svm_param_grid, scoring='recall', cv=3)\n",
    "svm_grid.fit(X_train, y_train)\n",
    "svm_model_best = svm_grid.best_estimator_\n",
    "\n",
    "# Grid search for KNN\n",
    "knn_grid = GridSearchCV(knn_model, param_grid=knn_param_grid, scoring='recall', cv=3)\n",
    "knn_grid.fit(X_train, y_train)\n",
    "knn_model_best = knn_grid.best_estimator_\n",
    "\n",
    "# Grid search for RandomForest\n",
    "rf_grid = GridSearchCV(rf_model, param_grid=rf_param_grid, scoring='recall', cv=3)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "rf_model_best = rf_grid.best_estimator_\n",
    "\n",
    "# Define the Voting Classifier with the selected base learners\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('svm', svm_model_best),\n",
    "        ('knn', knn_model_best),\n",
    "        ('rf', rf_model_best)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    weights=[1, 1, 2]\n",
    ")\n",
    "\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities and adjust the threshold for the test data\n",
    "voting_probs = voting_clf.predict_proba(X_test)[:, 1]\n",
    "threshold = 0.4\n",
    "voting_preds = (voting_probs > threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, voting_preds)\n",
    "recall = recall_score(y_test, voting_preds)\n",
    "precision = precision_score(y_test, voting_preds)\n",
    "f1 = f1_score(y_test, voting_preds)\n",
    "auc_roc = roc_auc_score(y_test, voting_probs)\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, voting_preds)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Output the metrics \n",
    "print(f\"Test Metrics:\")\n",
    "print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "print(f\"  Sensitivity: {sensitivity:.4f}\")\n",
    "print(f\"  Specificity: {specificity:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall: {recall:.4f}\")\n",
    "print(f\"  F1-Score: {f1:.4f}\")\n",
    "print(f\"  AUC ROC Score: {auc_roc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdb2c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Metrics obtained from above executed code\n",
    "metrics = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Sensitivity': sensitivity,\n",
    "    'Specificity': specificity,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'AUC ROC Score': auc_roc\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df_metrics = pd.DataFrame(list(metrics.items()), columns=['Metric', 'Value'])\n",
    "\n",
    "# Style the DataFrame\n",
    "styled_table = (\n",
    "    df_metrics.style\n",
    "    .set_table_styles([\n",
    "        {'selector': 'th', 'props': [('border', '2px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt'), ('font-weight', 'bold')]},  # Bold border and styling for headers\n",
    "        {'selector': 'td', 'props': [('border', '1px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt')]},  # Ordinary border and styling for cells\n",
    "        {'selector': 'caption', 'props': [('caption-side', 'top'), ('font-size', '14pt'), ('font-weight', 'bold'), ('color', 'black'), ('font-family', 'Times New Roman')]},  # Style for caption\n",
    "    ])\n",
    "    .set_properties(**{'border': '1px solid black'})  # Border for all cells\n",
    "    .set_caption(\"3 Base Learners: RF, SVM and KNN\")  # Add a caption to the table\n",
    ")\n",
    "\n",
    "# Remove the index column (row numbers)\n",
    "styled_table = styled_table.hide(axis=\"index\")\n",
    "\n",
    "styled_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194f0864",
   "metadata": {},
   "source": [
    "## Final Model with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import logging\n",
    "\n",
    "# Suppress XGBoost warnings\n",
    "logging.getLogger('xgboost').setLevel(logging.ERROR)\n",
    "\n",
    "# Set up cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "metrics = {\n",
    "    'accuracy': [],\n",
    "    'recall': [],\n",
    "    'precision': [],\n",
    "    'f1': [],\n",
    "    'auc_roc': [],\n",
    "    'sensitivity': [],\n",
    "    'specificity': []\n",
    "}\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, val_index in cv.split(X_train, y_train):\n",
    "    X_cv_train, X_cv_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_cv_train, y_cv_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Grid search for SVM\n",
    "    svm_grid = GridSearchCV(svm_model, param_grid=svm_param_grid, scoring='recall', cv=3)\n",
    "    svm_grid.fit(X_cv_train, y_cv_train)\n",
    "    svm_model_best = svm_grid.best_estimator_\n",
    "\n",
    "    # Grid search for RandomForest\n",
    "    rf_grid = GridSearchCV(rf_model, param_grid=rf_param_grid, scoring='recall', cv=3)\n",
    "    rf_grid.fit(X_cv_train, y_cv_train)\n",
    "    rf_model_best = rf_grid.best_estimator_\n",
    "\n",
    "    # Grid search for XGBoost\n",
    "    xgb_grid = GridSearchCV(xgb_model, param_grid=xgb_param_grid, scoring='recall', cv=3)\n",
    "    xgb_grid.fit(X_cv_train, y_cv_train)\n",
    "    xgb_model_best = xgb_grid.best_estimator_\n",
    "\n",
    "    # Grid search for k-NN\n",
    "    knn_grid = GridSearchCV(knn_model, param_grid=knn_param_grid, scoring='recall', cv=3)\n",
    "    knn_grid.fit(X_cv_train, y_cv_train)\n",
    "    knn_model_best = knn_grid.best_estimator_\n",
    "\n",
    "    # Defining the Voting Classifier\n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('svm', svm_model_best),\n",
    "            ('rf', rf_model_best),\n",
    "            ('xgb', xgb_model_best),\n",
    "            ('knn', knn_model_best)\n",
    "        ],\n",
    "        voting='soft',\n",
    "        weights=[0.5,1,2,0.5]\n",
    "    )\n",
    "\n",
    "    # Training the Voting Classifier\n",
    "    voting_clf.fit(X_cv_train, y_cv_train)\n",
    "\n",
    "    # Predicting probabilities and adjust the threshold\n",
    "    voting_probs = voting_clf.predict_proba(X_cv_val)[:, 1]\n",
    "    threshold = 0.4\n",
    "    voting_preds = (voting_probs > threshold).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_cv_val, voting_preds)\n",
    "    recall = recall_score(y_cv_val, voting_preds)\n",
    "    precision = precision_score(y_cv_val, voting_preds)\n",
    "    f1 = f1_score(y_cv_val, voting_preds)\n",
    "    auc_roc = roc_auc_score(y_cv_val, voting_probs)\n",
    "\n",
    "    # Confusion matrix to calculate sensitivity and specificity\n",
    "    conf_matrix = confusion_matrix(y_cv_val, voting_preds)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "    # Calculate Sensitivity and Specificity\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Store metrics for the fold\n",
    "    metrics['accuracy'].append(accuracy)\n",
    "    metrics['recall'].append(recall)\n",
    "    metrics['precision'].append(precision)\n",
    "    metrics['f1'].append(f1)\n",
    "    metrics['auc_roc'].append(auc_roc)\n",
    "    metrics['sensitivity'].append(sensitivity)\n",
    "    metrics['specificity'].append(specificity)\n",
    "\n",
    "    # Print metrics for the current fold\n",
    "    print(f\"Fold {len(metrics['accuracy'])}:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Sensitivity: {sensitivity:.4f}\")\n",
    "    print(f\"  Specificity: {specificity:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  AUC ROC Score: {auc_roc:.4f}\\n\")\n",
    "\n",
    "# Average metrics across all folds\n",
    "avg_accuracy = sum(metrics['accuracy']) / len(metrics['accuracy'])\n",
    "avg_recall = sum(metrics['recall']) / len(metrics['recall'])\n",
    "avg_precision = sum(metrics['precision']) / len(metrics['precision'])\n",
    "avg_f1 = sum(metrics['f1']) / len(metrics['f1'])\n",
    "avg_auc_roc = sum(metrics['auc_roc']) / len(metrics['auc_roc'])\n",
    "avg_sensitivity = sum(metrics['sensitivity']) / len(metrics['sensitivity'])\n",
    "avg_specificity = sum(metrics['specificity']) / len(metrics['specificity'])\n",
    "\n",
    "print(f\"\\nAverage Accuracy: {avg_accuracy:.4f}\")\n",
    "print(f\"Average Sensitivity: {avg_sensitivity:.4f}\")\n",
    "print(f\"Average Specificity: {avg_specificity:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average F1-Score: {avg_f1:.4f}\")\n",
    "print(f\"Average AUC ROC Score: {avg_auc_roc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c0c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Metrics obtained from your executed code\n",
    "metrics = {\n",
    "    'Accuracy': avg_accuracy,\n",
    "    'Sensitivity': avg_sensitivity,\n",
    "    'Specificity': avg_specificity,\n",
    "    'Precision': avg_precision,\n",
    "    'Recall': avg_recall,\n",
    "    'F1-Score': avg_f1,\n",
    "    'AUC ROC Score': avg_auc_roc\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df_metrics = pd.DataFrame(list(metrics.items()), columns=['Metric', 'Value'])\n",
    "\n",
    "# Style the DataFrame\n",
    "styled_table = (\n",
    "    df_metrics.style\n",
    "    .set_table_styles([\n",
    "        {'selector': 'th', 'props': [('border', '2px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt'), ('font-weight', 'bold')]},  # Bold border and styling for headers\n",
    "        {'selector': 'td', 'props': [('border', '1px solid black'), ('font-family', 'Times New Roman'), ('font-size', '12pt')]},  # Ordinary border and styling for cells\n",
    "        {'selector': 'caption', 'props': [('caption-side', 'top'), ('font-size', '14pt'), ('font-weight', 'bold'), ('color', 'black'), ('font-family', 'Times New Roman')]},  # Style for caption\n",
    "    ])\n",
    "    .set_properties(**{'border': '1px solid black'})  # Border for all cells\n",
    "    .set_caption(\"Final Model\")  # Add a caption to the table\n",
    ")\n",
    "\n",
    "# Remove the index column (row numbers)\n",
    "styled_table = styled_table.hide(axis=\"index\")\n",
    "\n",
    "# Display the styled table\n",
    "styled_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab07fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
